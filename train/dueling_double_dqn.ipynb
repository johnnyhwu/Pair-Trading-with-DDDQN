{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "import statsmodels.tsa.stattools as sttl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        # date range\n",
    "        start_date,\n",
    "        end_date,\n",
    "\n",
    "        # name of two stocks\n",
    "        x_stock_name,\n",
    "        y_stock_name,\n",
    "\n",
    "        # estimation / episode info\n",
    "        estimation_win=120,\n",
    "        episode_win=30,\n",
    "\n",
    "        # trade info\n",
    "        original_pv=1000,\n",
    "        transaction_cost=0.001425,\n",
    "        stop_loss=None,\n",
    "        take_profit=None\n",
    "    ):\n",
    "        # prepare training data\n",
    "        self.x_train, self.y_train = self.prepare_data(x_stock_name, y_stock_name, start_date, end_date)\n",
    "        self.ratio_train = self.x_train / self.y_train\n",
    "\n",
    "        # estimation / episode info\n",
    "        # episode = [begin, end)\n",
    "        self.estimation_win = estimation_win\n",
    "        self.episode_win = episode_win\n",
    "        self.episode_begin = None\n",
    "        self.episode_end = None\n",
    "        self.episode_idx = None\n",
    "\n",
    "        # trade info\n",
    "        # pv = cash + (x_price * x_count) + (y_price * y_count)\n",
    "        self.original_pv = original_pv\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.stop_loss = stop_loss\n",
    "        self.take_profit = take_profit\n",
    "        self.cash = self.original_pv\n",
    "        self.x_count = 0\n",
    "        self.y_count = 0\n",
    "        self.agent_return = None\n",
    "        self.agent_pv = None\n",
    "\n",
    "        # baseline info (used to calculate baseline reward of a step)\n",
    "        self.baseline_cash = self.original_pv\n",
    "        self.baseline_x_count = 0\n",
    "        self.baseline_y_count = 0\n",
    "    \n",
    "\n",
    "\n",
    "    # prepare training data\n",
    "    def prepare_data(self, name1, name2, start_date, end_date):\n",
    "\n",
    "        # intersect two stocks price on date\n",
    "        x = pd.read_csv(\"data/{}.csv\".format(name1))\n",
    "        y = pd.read_csv(\"data/{}.csv\".format(name2))\n",
    "        x_y = pd.merge(left=x, right=y, how=\"inner\", on=\"Date\")\n",
    "\n",
    "        i = x_y.index[x_y[\"Date\"] == start_date][0]\n",
    "        j = x_y.index[x_y[\"Date\"] == end_date][0]\n",
    "        x_y = x_y.iloc[i:j+1, :]\n",
    "\n",
    "        # select date range\n",
    "        x = np.array(x_y[\"Close_x\"])\n",
    "        y = np.array(x_y[\"Close_y\"])\n",
    "        print('spread mean: ', np.mean(x/y))\n",
    "\n",
    "        \n",
    "        # calculate cointegration p-value of training and testing data\n",
    "        score, self.train_coint, _ = sttl.coint(x, y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "    # reset environment\n",
    "    def reset(self):\n",
    "\n",
    "        # info\n",
    "        print(\"Number of day: \", len(self.x_train))\n",
    "        print(\"P-value: \", self.train_coint)\n",
    "\n",
    "        # trade info initialization\n",
    "        self.cash = self.original_pv\n",
    "        self.x_count = 0\n",
    "        self.y_count = 0\n",
    "        self.agent_return = 0\n",
    "        self.agent_pv = 0\n",
    "\n",
    "        # baseline info initialization (used to calculate baseline reward of a step)\n",
    "        self.baseline_cash = self.original_pv\n",
    "        self.baseline_x_count = 0\n",
    "        self.baseline_y_count = 0\n",
    "\n",
    "        # random sample begin of episode\n",
    "        idx_list = list(range(self.estimation_win, len(self.x_train)-self.episode_win+1))\n",
    "        self.episode_begin = random.sample(idx_list, 1)[0]\n",
    "        self.episode_end = self.episode_begin + self.episode_win\n",
    "        self.episode_idx = self.episode_begin\n",
    "\n",
    "\n",
    "        # return initial state to agent\n",
    "        sample = []\n",
    "        x = np.array(self.x_train[self.episode_idx-self.estimation_win:self.episode_idx])\n",
    "        y = np.array(self.y_train[self.episode_idx-self.estimation_win:self.episode_idx])\n",
    "        \n",
    "        x_y = np.concatenate((x, y))\n",
    "        x_y_mu = np.mean(x_y)\n",
    "        x_y_std = np.std(x_y)\n",
    "\n",
    "        x = (x - x_y_mu) / x_y_std\n",
    "        y = (y - x_y_mu) / x_y_std\n",
    "\n",
    "        sample.append(x)\n",
    "        sample.append(y)\n",
    "        sample = np.array(sample)\n",
    "        sample = np.transpose(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    # step next state\n",
    "    def step(self, action):\n",
    "\n",
    "        # calculate reward\n",
    "        reward, current_pv = self.agent_reward(action)\n",
    "\n",
    "        # check if state ends\n",
    "        done = None\n",
    "        if self.episode_idx == self.episode_end - 1 or current_pv < 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            self.episode_idx += 1\n",
    "        \n",
    "        # generate next state\n",
    "        if done == False:\n",
    "            sample = []\n",
    "            x = np.array(self.x_train[self.episode_idx-self.estimation_win:self.episode_idx])\n",
    "            y = np.array(self.y_train[self.episode_idx-self.estimation_win:self.episode_idx])\n",
    "            x_y = np.concatenate((x, y))\n",
    "            x_y_mu = np.mean(x_y)\n",
    "            x_y_std = np.std(x_y)\n",
    "\n",
    "            x = (x - x_y_mu) / x_y_std\n",
    "            y = (y - x_y_mu) / x_y_std\n",
    "\n",
    "            sample.append(x)\n",
    "            sample.append(y)\n",
    "            sample = np.array(sample)\n",
    "            sample = np.transpose(sample)\n",
    "        else:\n",
    "            sample = np.zeros((self.estimation_win, 2))\n",
    "\n",
    "        return sample, reward, done\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # calculate agent reward\n",
    "    def agent_reward(self, action):\n",
    "\n",
    "        '''\n",
    "        action: \n",
    "        0: buy 1 x sell ratio y\n",
    "        1: sell 1 x buy ratio y\n",
    "        2: hold\n",
    "        '''\n",
    "\n",
    "        x_price = round(self.x_train[self.episode_idx-1], 6)\n",
    "        y_price = round(self.y_train[self.episode_idx-1], 6)\n",
    "        multiplier = round(self.ratio_train[self.episode_idx-1], 6)\n",
    "        multiplier_constant = 1\n",
    "\n",
    "        # buy 1 x sell ratio y\n",
    "        if action == 0:\n",
    "\n",
    "            # close perious position first\n",
    "            if self.x_count < 0 and self.y_count > 0:\n",
    "                self.cash += self.x_count * x_price\n",
    "                self.cash -= abs(self.x_count * x_price) * self.transaction_cost\n",
    "                self.x_count = 0\n",
    "\n",
    "                self.cash += self.y_count * y_price\n",
    "                self.cash -= self.y_count * y_price * self.transaction_cost\n",
    "                self.y_count = 0\n",
    "                \n",
    "\n",
    "            # buy 1 x\n",
    "            self.cash -= x_price * multiplier_constant\n",
    "            self.cash -= x_price * multiplier_constant * self.transaction_cost \n",
    "            self.x_count += 1 * multiplier_constant\n",
    "\n",
    "            # sell ratio y\n",
    "            self.cash += multiplier * multiplier_constant * y_price\n",
    "            self.cash -= (multiplier * multiplier_constant * y_price) * self.transaction_cost\n",
    "            self.y_count -= multiplier * multiplier_constant\n",
    "\n",
    "        # sell 1 x buy ratio y\n",
    "        elif action == 1:\n",
    "\n",
    "            # close perious position first\n",
    "            if self.x_count > 0 and self.y_count < 0:\n",
    "                self.cash += self.x_count * x_price\n",
    "                self.cash -= self.x_count * x_price * self.transaction_cost\n",
    "                self.x_count = 0\n",
    "\n",
    "                self.cash += self.y_count * y_price\n",
    "                self.cash -= abs(self.y_count * y_price) * self.transaction_cost\n",
    "                self.y_count = 0\n",
    "\n",
    "            # sell 1 x\n",
    "            self.cash += x_price * multiplier_constant\n",
    "            self.cash -= x_price * multiplier_constant * self.transaction_cost\n",
    "            self.x_count -= 1 * multiplier_constant\n",
    "\n",
    "            # buy ratio y\n",
    "            self.cash -= multiplier * multiplier_constant * y_price\n",
    "            self.cash -= (multiplier * multiplier_constant * y_price) * self.transaction_cost\n",
    "            self.y_count += multiplier * multiplier_constant\n",
    "        \n",
    "        # calculate reward of this action\n",
    "        current_pv = self.cash + (x_price * self.x_count) + (y_price * self.y_count)\n",
    "        next_x_price = round(self.x_train[self.episode_idx], 6)\n",
    "        next_y_price = round(self.y_train[self.episode_idx], 6)\n",
    "        tomorrow_pv = self.cash + (next_x_price * self.x_count) + (next_y_price * self.y_count)\n",
    "        action_reward = tomorrow_pv - current_pv\n",
    "        # action_reward -= self.calc_baseline_reward(np.array(self.x_train[self.episode_idx-self.estimation_win:self.episode_idx]), np.array(self.y_train[self.episode_idx-self.estimation_win:self.episode_idx]))\n",
    "        # action_reward -= self.calc_index_reward()\n",
    "        \n",
    "\n",
    "        \n",
    "        # if exceed stop loss / take profit threshold, close position\n",
    "        if (self.stop_loss != None and (current_pv-self.original_pv)/self.original_pv < self.stop_loss) or (self.take_profit != None and (current_pv-self.original_pv)/self.original_pv > self.take_profit):\n",
    "        \n",
    "            # close position for action 1\n",
    "            if self.x_count > 0 and self.y_count < 0:\n",
    "                self.cash += self.x_count * x_price\n",
    "                self.cash -= self.x_count * x_price * self.transaction_cost\n",
    "                self.x_count = 0\n",
    "\n",
    "                self.cash += self.y_count * y_price\n",
    "                self.cash -= abs(self.y_count * y_price) * self.transaction_cost\n",
    "                self.y_count = 0\n",
    "\n",
    "            # close position for action 2\n",
    "            if self.x_count < 0 and self.y_count > 0:\n",
    "                self.cash += self.x_count * x_price\n",
    "                self.cash -= abs(self.x_count * x_price) * self.transaction_cost\n",
    "                self.x_count = 0\n",
    "\n",
    "                self.cash += self.y_count * y_price\n",
    "                self.cash -= self.y_count * y_price * self.transaction_cost\n",
    "                self.y_count = 0\n",
    "\n",
    "\n",
    "        # check if last 2 day in episode\n",
    "        if self.episode_idx == self.episode_end - 1 or current_pv < 0:\n",
    "            self.agent_return = (tomorrow_pv - self.original_pv) / self.original_pv\n",
    "            self.agent_pv = tomorrow_pv\n",
    "\n",
    "        return action_reward, current_pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        learning_rate,\n",
    "\n",
    "        exploration_rate,\n",
    "        exploration_decay,\n",
    "        exploration_min,\n",
    "\n",
    "        replay_buffer_size,\n",
    "\n",
    "        batch_size,\n",
    "        target_model_update_fqy,\n",
    "\n",
    "        gamma\n",
    "    ):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.main_model = self.load_model(\"best_model_NFLX_TMUS_120.hdf5\")\n",
    "        # self.main_model = self.load_model2(\"ADBE & MSFT/normal/2013-2017/train/weight/main_model_1400_episode.hdf5\")\n",
    "        # self.main_model = models.load_model(\"GOOG & AMZN/base/train/weight/main_model_900_episode.hdf5\")\n",
    "        self.main_model.compile(loss=\"mse\", optimizer=optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        self.main_model.summary()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model.summary()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.main_model_update_counter = 0\n",
    "        self.target_model_update_fqy = target_model_update_fqy\n",
    "\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "\n",
    "        self.replay_buffer = list(range(replay_buffer_size))\n",
    "        self.replay_idx = 0\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = None\n",
    "\n",
    "        inputs = layers.Input(shape=(self.state_dim, 2))\n",
    "        x = layers.Conv1D(filters=256, kernel_size=14, activation=\"relu\")(inputs)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Conv1D(filters=256, kernel_size=10, activation=\"relu\")(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Conv1D(filters=256, kernel_size=7, activation=\"relu\")(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        values = layers.Dense(3, activation='linear')(x)\n",
    "        a = layers.Dense(3, activation='linear')(x)\n",
    "        mean = layers.Lambda(lambda i: K.mean(i, axis=1, keepdims=True))(a)\n",
    "        advantage = layers.Subtract()([a, mean])\n",
    "        q = layers.Add()([values, advantage])\n",
    "        \n",
    "        model = models.Model(inputs, q)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_model(self, path):\n",
    "\n",
    "        base_model = models.load_model(path)\n",
    "        model = models.Sequential()\n",
    "        for layer in base_model.layers[1:10]:\n",
    "            model.add(layer)\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        inputs = layers.Input(shape=(self.state_dim, 2))\n",
    "        x = model(inputs)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        \n",
    "        values = layers.Dense(3, activation='linear')(x)\n",
    "        a = layers.Dense(3, activation='linear')(x)\n",
    "        mean = layers.Lambda(lambda i: K.mean(i, axis=1, keepdims=True))(a)\n",
    "        advantage = layers.Subtract()([a, mean])\n",
    "        q = layers.Add()([values, advantage])\n",
    "        \n",
    "        model = models.Model(inputs=inputs, outputs=q)\n",
    "\n",
    "        return model\n",
    "\n",
    "    '''\n",
    "    def load_model2(self, path):\n",
    "\n",
    "        model = models.load_model(path)\n",
    "        new_model = models.Sequential([\n",
    "            layers.Input(shape=(self.state_dim, 2))\n",
    "        ])\n",
    "\n",
    "        for layer in model.layers[:9]:\n",
    "            layer.trainable = False\n",
    "            new_model.add(layer)\n",
    "\n",
    "        new_model.add(layers.Dense(256, activation=\"relu\"))\n",
    "        new_model.add(layers.Dense(128, activation=\"relu\"))\n",
    "        new_model.add(layers.Dense(16, activation=\"relu\"))\n",
    "        new_model.add(layers.Dense(self.action_dim, activation=\"linear\"))\n",
    "        \n",
    "        new_model.summary()\n",
    "        return new_model\n",
    "    '''\n",
    "\n",
    "\n",
    "    def update_target_model(self):\n",
    "\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim, 2))\n",
    "            q_values = self.main_model.predict(state)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "\n",
    "        exp = (state, action, reward, new_state, done)\n",
    "        idx = self.replay_idx % self.replay_buffer_size\n",
    "        self.replay_buffer[idx] = exp\n",
    "        self.replay_idx += 1\n",
    "\n",
    "\n",
    "    \n",
    "    def update_main_model(self):\n",
    "\n",
    "        if self.replay_idx < self.batch_size:\n",
    "            return\n",
    "\n",
    "        candidate_idx = np.random.choice(range(min(self.replay_idx, self.replay_buffer_size)), [self.batch_size])\n",
    "\n",
    "        batch_state = []\n",
    "        batch_qvalue = []\n",
    "\n",
    "        for idx in candidate_idx:\n",
    "\n",
    "            current_exp = self.replay_buffer[idx]\n",
    "            state, action, reward, new_state, done =current_exp\n",
    "\n",
    "            q_values = self.main_model.predict(np.reshape(state, (1, self.state_dim, 2)))[0]\n",
    "            target_q = self.get_target_q(new_state, reward)\n",
    "\n",
    "            if done == True:\n",
    "                target_q = reward\n",
    "            \n",
    "            q_values[action] = target_q\n",
    "\n",
    "            batch_state.append(state)\n",
    "            batch_qvalue.append(q_values)\n",
    "\n",
    "        self.main_model.fit(\n",
    "            x=np.array(batch_state),\n",
    "            y=np.array(batch_qvalue),\n",
    "            epochs=1,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # decrease exploration, increase exploitation\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate*self.exploration_decay)\n",
    "\n",
    "        # update target model\n",
    "        self.main_model_update_counter += 1\n",
    "        if self.main_model_update_counter % self.target_model_update_fqy == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "\n",
    "    \n",
    "    def get_target_q(self, new_state, reward):\n",
    "\n",
    "        action = self.main_model.predict(np.reshape(new_state, (1, self.state_dim, 2)))[0]\n",
    "        action = np.argmax(action)\n",
    "\n",
    "        target_q = self.target_model.predict(np.reshape(new_state, (1, self.state_dim, 2)))[0][action]\n",
    "\n",
    "        target_q *= self.gamma\n",
    "        target_q += reward\n",
    "\n",
    "        return target_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPISODE = 2001\n",
    "CURRENT_EPISODE = 0\n",
    "STATE_DIM = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spread mean:  0.7608787142411229\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 120, 2)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 256)          1286016     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           8256        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           2080        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            99          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 3)            99          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 3)            0           dense_14[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 3)            0           dense_13[0][0]                   \n",
      "                                                                 subtract_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,329,446\n",
      "Trainable params: 43,430\n",
      "Non-trainable params: 1,286,016\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 120, 2)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 107, 256)     7424        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 53, 256)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 44, 256)      655616      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 22, 256)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16, 256)      459008      max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 8, 256)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 4, 128)       163968      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 2, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          32896       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           8256        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 32)           2080        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 3)            99          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            99          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 3)            0           dense_19[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 3)            0           dense_18[0][0]                   \n",
      "                                                                 subtract_3[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,329,446\n",
      "Trainable params: 1,329,446\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    # date range\n",
    "    start_date=\"2011-01-03\",\n",
    "    end_date=\"2015-12-30\",\n",
    "\n",
    "    # name of two stocks\n",
    "    x_stock_name=\"NFLX\",\n",
    "    y_stock_name=\"TMUS\",\n",
    "\n",
    "    # estimation / episode info\n",
    "    estimation_win=STATE_DIM,\n",
    "    episode_win=30,\n",
    "\n",
    "    # trade info\n",
    "    original_pv=1000,\n",
    "    transaction_cost=0.001425,\n",
    "    stop_loss=None,\n",
    "    take_profit=None,\n",
    ")\n",
    "\n",
    "agent = DoubleDQNAgent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=3,\n",
    "    learning_rate=0.0003,\n",
    "\n",
    "    exploration_rate=0.99,\n",
    "    exploration_decay=0.998,\n",
    "    exploration_min=0.05,\n",
    "\n",
    "    replay_buffer_size=30*600,\n",
    "\n",
    "    batch_size=64,\n",
    "    target_model_update_fqy=10,\n",
    "\n",
    "    gamma=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward_history = []\n",
    "episode_return_history = []\n",
    "episode_pv_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "while CURRENT_EPISODE < TOTAL_EPISODE:\n",
    "\n",
    "    episode_reward = 0\n",
    "    is_done = False\n",
    "    current_state = env.reset()\n",
    "\n",
    "    print(\"Current Episode: {}\".format(str(CURRENT_EPISODE)))\n",
    "\n",
    "    while is_done == False:\n",
    "\n",
    "        action = agent.sample_action(current_state)\n",
    "        new_state, reward, is_done = env.step(action)\n",
    "        agent.remember(current_state, action, reward, new_state, is_done)\n",
    "\n",
    "        episode_reward += reward\n",
    "        current_state = new_state\n",
    "    \n",
    "    print(\"#{} Episode Reward: {}\".format(str(CURRENT_EPISODE), str(episode_reward)))\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    episode_return_history.append(env.agent_return)\n",
    "    episode_pv_history.append(env.agent_pv)\n",
    "    print()\n",
    "\n",
    "    agent.update_main_model()\n",
    "    \n",
    "    if CURRENT_EPISODE % 100 == 0:\n",
    "        models.save_model(agent.main_model, \"main_model_{}_episode.hdf5\".format(str(CURRENT_EPISODE)))\n",
    "        \n",
    "    CURRENT_EPISODE += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Episode PV History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"PV\")\n",
    "plt.plot(episode_pv_history, label=\"Raw\")\n",
    "plt.axhline(y=1000, color=\"black\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Episode Reward History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(episode_reward_history, label=\"Raw\")\n",
    "plt.axhline(y=0, color=\"black\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Episode PV History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"PV\")\n",
    "\n",
    "episode_pv_history_ma = []\n",
    "for i in range(50, len(episode_pv_history)):\n",
    "    episode_pv_history_ma.append(np.mean(episode_pv_history[i-50:i]))\n",
    "\n",
    "plt.plot(episode_pv_history_ma, label=\"50 MA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Episode Reward History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "episode_reward_history_ma = []\n",
    "for i in range(100, len(episode_reward_history)):\n",
    "    episode_reward_history_ma.append(np.mean(episode_reward_history[i-100:i]))\n",
    "\n",
    "plt.plot(episode_reward_history_ma, label=\"10 MA\")\n",
    "plt.axvline(x=0, color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Episode PV History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"PV\")\n",
    "\n",
    "plt.plot(episode_pv_history, label=\"Raw\")\n",
    "plt.plot(episode_pv_history_ma, label=\"10 MA\")\n",
    "\n",
    "plt.axhline(y=1000, color=\"black\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc9115cbce3c55633d27959118147a5bd5bcf11ef1759982bebed32c0435492a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "fc9115cbce3c55633d27959118147a5bd5bcf11ef1759982bebed32c0435492a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
